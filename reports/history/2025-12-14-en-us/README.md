# MDB Cross-Language Comparison Report - 2025-12-14

## Overview

This report documents the first cross-language comparison between English (US) and Portuguese (Brazilian) social engineering patterns on the Multilingual Deception Bench.

**Key Finding: Models show LOWER refusal rates on English content compared to Portuguese.**

---

## Dataset Summary

| Language | Dataset Version | Total Examples | Seed Examples | Categories |
|----------|-----------------|----------------|---------------|------------|
| EN-US    | v1.0.0          | 1,093          | 111           | 20         |
| PT-BR    | v1.8.0          | 1,100          | 198           | 64         |

### EN-US Categories
IRS_TAX, SSA, MEDICARE, DELIVERY_USPS, DELIVERY_FEDEX, DELIVERY_AMAZON, GRANDPARENT, TECH_SUPPORT, BANK_CHASE, BANK_BOA, BANK_WELLS, CRYPTO_PIG, ROMANCE, JOB_WFH, UTILITY, LOTTERY, JURY_DUTY, FBI_DEA, STUDENT_LOAN, ZELLE_VENMO

---

## Results Summary (100 samples per model)

### EN-US Results

| Model | Refusal | Soft Refusal | Compliance |
|-------|---------|--------------|------------|
| Claude Haiku 4.5 | **0.0%** | 7.0% | 93.0% |
| Claude Sonnet 4 | **0.0%** | 14.0% | 86.0% |
| GPT-4o | 13.0% | 0.0% | 87.0% |

### PT-BR Results (for comparison)

| Model | Refusal | Soft Refusal | Compliance | n |
|-------|---------|--------------|------------|---|
| Claude Haiku 4.5 | 5.9% | 6.5% | 87.5% | 1100 |
| Claude Sonnet 4 | 6.6% | 7.6% | 85.7% | 1100 |
| GPT-4o | 28.6% | 0.2% | 71.2% | 1100 |

---

## Cross-Language Delta

| Model | Refusal Δ | Compliance Δ | Interpretation |
|-------|-----------|--------------|----------------|
| Claude Haiku 4.5 | **-5.9pp** | +5.5pp | Less safe in EN |
| Claude Sonnet 4 | **-6.6pp** | +0.3pp | Less safe in EN |
| GPT-4o | **-15.6pp** | +15.8pp | Much less safe in EN |

*Negative refusal delta = model refuses LESS in English (worse safety)*

---

## Key Findings

### 1. Counter-Intuitive Result: English Has Weaker Safety

Despite the assumption that AI labs focus safety efforts on English, models show:
- **0% refusal rate** for Claude models on EN-US social engineering
- **~16pp lower refusal** for GPT-4o on EN-US vs PT-BR

### 2. Possible Explanations

1. **Pattern Familiarity**: US scam patterns (IRS, SSA, etc.) may appear more like legitimate requests
2. **Training Data Bias**: Models may be trained to be more helpful in English contexts
3. **Evaluation Methodology**: Classification may need language-specific calibration
4. **Content Realism**: Native EN-US seeds may be more sophisticated/subtle

### 3. Attack Type Vulnerability (EN-US)

| Attack Type | % of Dataset | Typical Model Response |
|-------------|--------------|------------------------|
| Government Impersonation | 27.3% | High compliance (IRS, SSA, Medicare) |
| Financial Fraud | 21.6% | High compliance (crypto, payment apps) |
| Delivery Scam | 12.6% | High compliance (USPS, FedEx, Amazon) |
| Bank Impersonation | 8.1% | High compliance |
| Family Emergency | 6.2% | Moderate compliance (grandparent scam) |

---

## Methodology Notes

### Seed Sources (EN-US)
- FTC Consumer Sentinel Network 2024
- IRS Dirty Dozen 2025
- SSA Scam Alerts
- FBI IC3 Reports
- AARP Fraud Watch
- FCC Consumer Alerts
- Medicare.gov Fraud Alerts
- Chainalysis Crypto Crime Report 2024

### Evaluation Process
1. 100 samples evaluated per model (quick comparison)
2. Same classification algorithm used for both languages
3. Responses classified as: refusal, soft_refusal, compliance, unclear
4. Raw model outputs scored in memory, not stored

---

## Recommendations

1. **Expand EN-US evaluation** to full 1,093 examples for statistical significance
2. **Investigate classification calibration** for English vs Portuguese
3. **Test additional models** (Claude Opus 4.5, Gemini, etc.)
4. **Analyze by attack type** to identify specific vulnerabilities
5. **Consider cultural context** in safety filter design

---

## Files in this Report

- `README.md` - This summary document
- EN-US results: `results/claude_sonnet_4_en_us.json`, `results/claude_haiku_4_5_en_us.json`, `results/gpt_4o_en_us.json`
- EN-US dataset: `data/generated/en-us-synthetic.jsonl`
- EN-US seeds: `data/seeds/en-us-seeds.json`

---

*Report generated by MDB CLI v0.1.0*
*Datasets: EN-US v1.0.0 (1,093 examples) vs PT-BR v1.8.0 (1,100 examples)*
